{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Subject averaging.\n",
    "\n",
    "## Goals:\n",
    "\n",
    "Subjects viewed two slightly different types of video. Rather than averaging the data across all subjects, it is better to do due dilligence and average them depending on the specific video that they viewed. \n",
    "\n",
    "I want to determine who viewed what video, average accordingly and then save in the appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tank/hedger/software/hcp_movie/cfhcpy/base.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/tank/hedger/software/hcp_movie/cfhcpy/fit_utils.py:22: UserWarning: \n",
      "\n",
      " | Using Nistats with Nilearn versions >= 0.7.0 is redundant and potentially conflicting.\n",
      " | Nilearn versions 0.7.0 and up offer all the functionality of Nistats as well the latest features and fixes.\n",
      " | We strongly recommend uninstalling Nistats and using Nilearn's stats & reporting modules.\n",
      "\n",
      "  from nistats.hemodynamic_models import spm_hrf\n",
      "/tank/hedger/software/anaconda3/envs/p3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.linear_model.ridge module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from cfhcpy.base import AnalysisBase\n",
    "ab = AnalysisBase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start an analysis base with an arbitrary subject, just so we have access to all the paths etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import HCP_subject\n",
    "subno=114823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of subject 114823 on romulus with settings \n",
      "{\n",
      " \"identifier\": \"node230\",\n",
      " \"base_dir\": \"/tank/shared/2020/hcp_{experiment}/\",\n",
      " \"code_dir\": \"/tank/hedger/scripts/HCP_tonotopy\",\n",
      " \"threads\": 40\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ab.startup(subject=str(subno), experiment_id='movie', yaml_file='/tank/hedger/software/hcp_movie/config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ab.full_data_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wc_exp': 'MOVIE',\n",
       " 'runs': [1, 2, 3, 4],\n",
       " 'test_duration': 103,\n",
       " 'run_durations': [921, 918, 915, 901],\n",
       " 'data_file_wildcard': 'tfMRI_{experiment_id}{run}_*_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc.nii'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.experiment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through all the subjects, use their CSV file to return the video they watched. Dump this in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movtype=[]\n",
    "dpaths=[]\n",
    "from tqdm import tqdm\n",
    "\n",
    "for sub in tqdm(ab.full_data_subjects):\n",
    "    tbase = AnalysisBase()\n",
    "    tbase.startup(subject=str(sub), experiment_id='movie', yaml_file='/tank/hedger/software/hcp_movie/config.yml')\n",
    "    mysub=HCP_subject(tbase)\n",
    "    movtype.append(mysub.vidprefix)\n",
    "    mysub.get_data_paths()\n",
    "    dpaths.append(mysub.dpaths)\n",
    "    tbase=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['movies/Post_20140821_version/', 'movies/Pre_20140821_version/'],\n",
       "      dtype='<U29')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(movtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['movies/Post_20140821_version/', 'movies/Pre_20140821_version/'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(movtype).keys() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of participants that viewed each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([132, 42])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(movtype).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem that most viewed the new video. Get the indices of the participants that viewed each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyvidsubs= [i for i, s in enumerate(movtype) if 'Pre_20140821' in s]\n",
    "latevidsubs= [i for i, s in enumerate(movtype) if 'Post_20140821' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_array(dpaths,s,movind):\n",
    "    data=nib.load(dpaths[s][movind])\n",
    "    darray=np.array(data.get_data())\n",
    "    return darray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to output the early and late subjects to different directories. Do this in my own space so as to not disrupt the original folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifti\n",
    "earlydir='/tank/hedger/DATA/HCP_temp/early'\n",
    "latedir='/tank/hedger/DATA/HCP_temp/late'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might as well preserve the same kind of filename so that all of the HCP_movie functionality will still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEDS=['AP','PA','PA','AP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "def create_mean_data(dpaths,subs,movienum,outputdir,save):\n",
    "    \n",
    "    sub_readin=Parallel(n_jobs=20,verbose=1)(delayed(read_array)(dpaths,s,movienum)  for s in subs) #Read in all the data\n",
    "    sub_array=np.array(sub_readin) # Make into big array\n",
    "    sub_meandat=np.mean(sub_array,0) # Take mean.\n",
    "    \n",
    "    if save==True:\n",
    "        \n",
    "        tps=np.array(range(ab.experiment_dict['run_durations'][movienum])).astype('str') # The cifti writing seems to require us to give the number of timepoints\n",
    "        \n",
    "        fname=os.path.join(outputdir,'tfMRI_{experiment_id}{run}_{PED}_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc.nii'.format(experiment_id='MOVIE',run=str(ab.experiment_dict['runs'][movienum]),PED=PEDS[movienum]))        \n",
    "        \n",
    "        \n",
    "        cifti.write(fname, sub_meandat,(cifti.Scalar.from_names(tps),ab.cifti_brain_model))\n",
    "    \n",
    "    return sub_meandat,fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go though the early and late subjects, save the mean volume for each movie and then split it into surface giftis and subcortex data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfhcpy.surf_utils import split_cii\n",
    "\n",
    "def split_dfile(f2split):\n",
    "        # Summons workbench to split a data file\n",
    "        nfileL,nfileR=f2split[:-4]+'_L.gii',f2split[:-4]+'_R.gii'\n",
    "        if not os.path.isfile(nfileL):\n",
    "            result=split_cii(fn=f2split,workbench_split_command=ab.workbench_split_command,resample=False)\n",
    "        else:\n",
    "            print(f'split files already exist subject {ab.subject}','Skipping..')\n",
    "        \n",
    "        return nfileL,nfileR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  42 out of  42 | elapsed:   50.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE1_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE1_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,earlyvidsubs,0,earlydir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  42 out of  42 | elapsed:   53.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE2_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE2_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,earlyvidsubs,1,earlydir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  42 out of  42 | elapsed:   57.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE3_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE3_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,earlyvidsubs,2,earlydir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  42 out of  42 | elapsed:   50.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE4_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE4_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,earlyvidsubs,3,earlydir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=20)]: Done 132 out of 132 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE1_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE1_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,latevidsubs,0,latedir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=20)]: Done 132 out of 132 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE2_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE2_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,latevidsubs,1,latedir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=20)]: Done 132 out of 132 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE3_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE3_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,latevidsubs,2,latedir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=20)]: Done 132 out of 132 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE4_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE4_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,latevidsubs,3,latedir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
