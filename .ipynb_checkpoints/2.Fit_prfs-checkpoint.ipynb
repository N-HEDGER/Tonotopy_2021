{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PRF fitting\n",
    "\n",
    "## Goals\n",
    "\n",
    "1. Perform CSS model fitting on both the early and late subject we created.\n",
    "2. Save the data out to a hdf file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tank/hedger/software/hcp_movie/cfhcpy/base.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/tank/hedger/software/hcp_movie/cfhcpy/fit_utils.py:22: UserWarning: \n",
      "\n",
      " | Using Nistats with Nilearn versions >= 0.7.0 is redundant and potentially conflicting.\n",
      " | Nilearn versions 0.7.0 and up offer all the functionality of Nistats as well the latest features and fixes.\n",
      " | We strongly recommend uninstalling Nistats and using Nilearn's stats & reporting modules.\n",
      "\n",
      "  from nistats.hemodynamic_models import spm_hrf\n",
      "/tank/hedger/software/anaconda3/envs/p3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.linear_model.ridge module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from cfhcpy.base import AnalysisBase\n",
    "\n",
    "import cortex\n",
    "import numpy as np\n",
    "from funcs import HCP_subject, h5_make, h5_dump2, hdfshutter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from prfpy.fit import Iso1DGaussianFitter\n",
    "from prfpy.model import Iso1DGaussianModel\n",
    "from prfpy.stimulus import PRFStimulus1Dn\n",
    "from prfpy.model import CSS_Iso1DGaussianModel\n",
    "from prfpy.fit import CSS_Iso1DGaussianFitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start an analysis base and read in the late subject we defined in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of subject late on romulus with settings \n",
      "{\n",
      " \"identifier\": \"node230\",\n",
      " \"base_dir\": \"/scratch/2019/visual/hcp_{experiment}/\",\n",
      " \"code_dir\": \"/tank/hedger/scripts/HCP_tonotopy\",\n",
      " \"threads\": 40\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "late = AnalysisBase()\n",
    "\n",
    "late.startup(subject='late', experiment_id='movie', yaml_file='/tank/hedger/software/hcp_movie/config.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this lives outside the HCP dataset, we need to modify the base directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "late.subject_base_dir='/tank/hedger/DATA/HCP_temp/late'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of subject late on romulus with settings \n",
      "{\n",
      " \"identifier\": \"node230\",\n",
      " \"base_dir\": \"/scratch/2019/visual/hcp_{experiment}/\",\n",
      " \"code_dir\": \"/tank/hedger/scripts/HCP_tonotopy\",\n",
      " \"threads\": 40\n",
      "}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Prompt : After ref date? 1=True or 0=False 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:19<00:00, 19.89s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating design matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/921 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 921/921 [00:00<00:00, 6585.83it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/921 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 921/921 [00:00<00:00, 6675.72it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:57<02:52, 57.52s/it]\n",
      "  0%|          | 0/918 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 918/918 [00:00<00:00, 6618.07it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/918 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 918/918 [00:00<00:00, 6587.42it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [02:22<02:11, 65.80s/it]\n",
      "  0%|          | 0/915 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 915/915 [00:00<00:00, 6581.90it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/915 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 915/915 [00:00<00:00, 6438.15it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [03:58<01:14, 74.79s/it]\n",
      "  0%|          | 0/901 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 901/901 [00:00<00:00, 6699.83it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/901 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 901/901 [00:00<00:00, 5623.45it/s]\u001b[A\n",
      "100%|██████████| 4/4 [05:34<00:00, 83.56s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making training and test folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:38<00:00, 24.53s/it]\n"
     ]
    }
   ],
   "source": [
    "late = AnalysisBase()\n",
    "\n",
    "late.startup(subject='late', experiment_id='movie', yaml_file='/tank/hedger/software/hcp_movie/config.yml')\n",
    "late.subject_base_dir='/tank/hedger/DATA/HCP_temp/late'\n",
    "\n",
    "latesub=HCP_subject(late)\n",
    "\n",
    "latesub.prep_data()\n",
    "\n",
    "dtype='main' # Use the independent data, thereby chopping off the 'test sequence'\n",
    "standardise=True # Standardise the design matrix.\n",
    "zaxis=1\n",
    "filt=False # Dont filter the design matrix, we will filter the predictions instead.\n",
    "\n",
    "\n",
    "latesub.import_data(dtype,standardise,filt,zaxis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the bounds for mu and sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func='cart'\n",
    "mu_min=np.log(88) # Use same frequency values as Thomas et al., 2015\n",
    "mu_max=np.log(8000)\n",
    "mu_steps=50\n",
    "\n",
    "\n",
    "sigma_min=.02\n",
    "sigma_max=4\n",
    "sigma_steps=20\n",
    "\n",
    "# Savgol filter params.\n",
    "fparams = {\n",
    "  \"window_length\":201,\n",
    "  \"polyorder\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the fitting on the full brain (all 118584 vertices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymask=np.ones(118584)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_subject(sub,mu_min,mu_max,mu_steps,sigma_min,sigma_max,sigma_steps,func,njobs,verbose=False,log='natural',mask=mymask):\n",
    "    \n",
    "    if log=='natural':\n",
    "        \n",
    "        frequencies=np.log(sub.frequencies) # If we want the gaussian to be defined in log frequency, as per Tomas et al.\n",
    "        \n",
    "    elif log=='log10':\n",
    "        \n",
    "        frequencies=np.log10(sub.frequencies)\n",
    "        \n",
    "    else:\n",
    "        frequencies=sub.frequencies\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    CSS_fitted_params=[] # Make list to populate with CSS params.\n",
    "    CSS_xval_R2=[] # Make list to populate with CSS xval performance.\n",
    "    \n",
    "    for fold in tqdm(range(len(sub.dm_train))): # Loop that goes through cross validation folds.\n",
    "        \n",
    "        # We set up a vector that defines each run. This way we can perform the sg filtering on a run-by-run basis.\n",
    "        blengths=np.array(sub.ab.experiment_dict['run_durations'])[np.array(sub.folds[fold])]-sub.ab.experiment_dict['test_duration']\n",
    "        blockarr=np.repeat(sub.folds[fold],blengths)\n",
    "        \n",
    "        # Create training stimulus.\n",
    "        train_stim=PRFStimulus1Dn(sub.dm_train[fold],frequencies,TR=1,block_inds=blockarr) # Get the train data.\n",
    "        \n",
    "        # Create test stimulus.\n",
    "        test_stim=PRFStimulus1Dn(sub.dm_test[fold],frequencies,TR=1)\n",
    "        \n",
    "        # Make model using the train stimulus.\n",
    "        mymod=Iso1DGaussianModel(train_stim,normalise_RFs=False,filter_predictions=True,filter_type='sg',filter_params=fparams)\n",
    "        \n",
    "        mymod.create_grid_predictions(mu_min,mu_max,mu_steps,sigma_min,sigma_max,sigma_steps,func)\n",
    "        \n",
    "        \n",
    "        # Initial Gaussian pRF fitting.\n",
    "        print('grid fitting')\n",
    "        gf = Iso1DGaussianFitter(data=sub.data_train[fold].T,model=mymod, n_jobs=njobs)\n",
    "        gf.grid_fit(mu_min,mu_max,mu_steps,sigma_min,sigma_max,sigma_steps,func,verbose=True,n_batches=100)\n",
    "        \n",
    "        \n",
    "        # Do iterative fitting with gauss pRF.\n",
    "        print('iterative fitting')\n",
    "        bounds=((mu_min,mu_max),(sigma_min,sigma_max),(None,None),(None,None))\n",
    "        gf.rsq_mask=mask.astype(bool)\n",
    "        gf.iterative_fit(rsq_threshold=0,verbose=True,bounds=bounds)\n",
    "        \n",
    "        \n",
    "        # Do cross validation on gauss pRF.\n",
    "        print('cross validation')\n",
    "        gf.xval(test_data=sub.data_test[fold].T,test_stimulus=test_stim)\n",
    "        \n",
    "        # CSS fiting - define CSS model.\n",
    "        print('CSS fitting')\n",
    "        CSSmod=CSS_Iso1DGaussianModel(train_stim,normalise_RFs=False,filter_predictions=True,filter_type='sg',filter_params=fparams)\n",
    "        CSSmod.func=func\n",
    "        \n",
    "        # Define a CSS model that takes the gaussian params from the previous fitter as starting values.\n",
    "        CSS_extender=CSS_Iso1DGaussianFitter(model=CSSmod,data=sub.data_train[fold].T, n_jobs=njobs, fit_hrf=False,previous_gaussian_fitter=gf)\n",
    "        bounds=((mu_min,mu_max),(0,sigma_max),(None,None),(None,None),(0.01,2))\n",
    "        \n",
    "        # Do the iterative fitting. Just fit everything. Don't threshold based on R2. \n",
    "        CSS_extender.iterative_fit(rsq_threshold=0,verbose=True,bounds=bounds)\n",
    "        \n",
    "        # CSS cross-validation\n",
    "        print('CSS cross validation')\n",
    "        CSS_extender.xval(test_data=sub.data_test[fold].T,test_stimulus=test_stim)\n",
    "\n",
    "        # Bundle params and xval R2 into a list.\n",
    "        CSS_xval_R2.append(CSS_extender.CV_R2) \n",
    "        CSS_fitted_params.append(CSS_extender.iterative_search_params)\n",
    "        \n",
    "        # Empty out all of the parameters for the next fold.\n",
    "        gf, mymod,CSSmod, train_stim, test_stim,CSS_extender=None, None, None, None, None, None\n",
    "    \n",
    "    \n",
    "    # Make into arrays\n",
    "    CSS_cvs=np.array(CSS_xval_R2)\n",
    "    CSS_cvs[CSS_cvs == 0] = np.nan\n",
    "    \n",
    "    CSS_params=np.array(CSS_fitted_params)\n",
    "    CSS_params[CSS_params == 0] = np.nan\n",
    "    \n",
    "\n",
    "    return CSS_params,CSS_cvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform fitting on the late subject. This takes a while.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5_make('/tank/hedger/DATA/HCP_temp/late','AUDITORY_FITS_CSS_FULL')\n",
    "res=None\n",
    "\n",
    "res=analyze_subject(latesub,mu_min,mu_max,mu_steps,sigma_min,sigma_max,sigma_steps,func,njobs=30,log='natural',mask=mymask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dump2(f,res[0],'wb_log_fold_params_CSS')\n",
    "\n",
    "h5_dump2(f,res[1],'wb_log_fold_xval_CSS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same analysis for the early subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latesub=None\n",
    "early = AnalysisBase()\n",
    "\n",
    "early.startup(subject='early', experiment_id='movie', yaml_file='/tank/hedger/software/hcp_movie/config.yml')\n",
    "early.subject_base_dir='/tank/hedger/DATA/HCP_temp/early'\n",
    "\n",
    "earlysub=HCP_subject(early)\n",
    "\n",
    "earlysub.prep_data()\n",
    "\n",
    "earlysub.import_data(dtype,standardise,filt,zaxis)\n",
    "\n",
    "\n",
    "f=h5_make('/tank/hedger/DATA/HCP_temp/early','AUDITORY_FITS_CSS_FULL')\n",
    "res=None\n",
    "\n",
    "res=analyze_subject(earlysub,mu_min,mu_max,mu_steps,sigma_min,sigma_max,sigma_steps,func,njobs=30,log='natural',mask=mymask)\n",
    "\n",
    "h5_dump2(f,res[0],'wb_log_fold_params_CSS')\n",
    "\n",
    "h5_dump2(f,res[1],'wb_log_fold_xval_CSS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
