{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Subject averaging.\n",
    "\n",
    "## Goals:\n",
    "\n",
    "Subjects viewed two slightly different types of video as described in the HCP 12000 reference manual (https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf). Rather than averaging the data across all subjects, it is better to do due dilligence and average them depending on the specific video that they viewed (even if the differences are minor). \n",
    "\n",
    "This allows us to investigate the agreement of the parameters estimated from two across-subject folds.\n",
    "\n",
    "I want to determine which subject viewed what video, average accordingly and then save in the appropriate format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tank/hedger/software/hcp_movie/cfhcpy/base.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/tank/hedger/software/hcp_movie/cfhcpy/fit_utils.py:22: UserWarning: \n",
      "\n",
      " | Using Nistats with Nilearn versions >= 0.7.0 is redundant and potentially conflicting.\n",
      " | Nilearn versions 0.7.0 and up offer all the functionality of Nistats as well the latest features and fixes.\n",
      " | We strongly recommend uninstalling Nistats and using Nilearn's stats & reporting modules.\n",
      "\n",
      "  from nistats.hemodynamic_models import spm_hrf\n",
      "/tank/hedger/software/anaconda3/envs/p3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.linear_model.ridge module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from cfhcpy.base import AnalysisBase\n",
    "from funcs import HCP_subject\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import nibabel as nib\n",
    "import os\n",
    "import cifti\n",
    "from cfhcpy.surf_utils import split_cii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start an analysis base with an arbitrary subject, just so we have access to all the paths etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AnalysisBase()\n",
    "subno=114823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of subject 114823 on romulus with settings \n",
      "{\n",
      " \"identifier\": \"node230\",\n",
      " \"base_dir\": \"/scratch/2019/visual/hcp_{experiment}/\",\n",
      " \"code_dir\": \"/tank/hedger/scripts/HCP_tonotopy\",\n",
      " \"threads\": 40\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ab.startup(subject=str(subno), experiment_id='movie', yaml_file='/tank/hedger/software/hcp_movie/config.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through all the subjects, use their CSV file to return the video they watched. Dump this in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movtype=[]\n",
    "dpaths=[]\n",
    "\n",
    "for sub in tqdm(ab.full_data_subjects):\n",
    "    tbase = AnalysisBase()\n",
    "    tbase.startup(subject=str(sub), experiment_id='movie', yaml_file='/tank/hedger/software/hcp_movie/config.yml')\n",
    "    mysub=HCP_subject(tbase)\n",
    "    movtype.append(mysub.vidprefix)\n",
    "    mysub.get_data_paths()\n",
    "    dpaths.append(mysub.dpaths)\n",
    "    tbase=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of participants that viewed each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([132, 42])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(movtype).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem that most viewed the new video. Get the indices of the participants that viewed each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyvidsubs= [i for i, s in enumerate(movtype) if 'Pre_20140821' in s]\n",
    "latevidsubs= [i for i, s in enumerate(movtype) if 'Post_20140821' in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for reading in the data for a specific run/movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_array(dpaths,s,movind):\n",
    "    data=nib.load(dpaths[s][movind])\n",
    "    darray=np.array(data.get_data())\n",
    "    return darray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to output the early and late subjects to different directories. Do this in my own space so as to not disrupt the original folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlydir='/tank/hedger/DATA/HCP_temp/early'\n",
    "latedir='/tank/hedger/DATA/HCP_temp/late'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might as well preserve the same kind of filename so that all of the HCP_movie functionality will still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEDS=['AP','PA','PA','AP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for creating the mean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mean_data(dpaths,subs,movienum,outputdir,save):\n",
    "    \n",
    "    sub_readin=Parallel(n_jobs=20,verbose=1)(delayed(read_array)(dpaths,s,movienum)  for s in subs) #Read in all the data\n",
    "    sub_array=np.array(sub_readin) # Make into big array\n",
    "    sub_meandat=np.mean(sub_array,0) # Take mean.\n",
    "    \n",
    "    if save==True:\n",
    "        \n",
    "        tps=np.array(range(ab.experiment_dict['run_durations'][movienum])).astype('str') # The cifti writing seems to require us to give the number of timepoints\n",
    "        \n",
    "        fname=os.path.join(outputdir,'tfMRI_{experiment_id}{run}_{PED}_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc.nii'.format(experiment_id='MOVIE',run=str(ab.experiment_dict['runs'][movienum]),PED=PEDS[movienum]))        \n",
    "        \n",
    "        \n",
    "        cifti.write(fname, sub_meandat,(cifti.Scalar.from_names(tps),ab.cifti_brain_model))\n",
    "    \n",
    "    return sub_meandat,fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for splitting the data into the two cortical surfaces using workbench commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dfile(f2split):\n",
    "        # Summons workbench to split a data file\n",
    "        nfileL,nfileR=f2split[:-4]+'_L.gii',f2split[:-4]+'_R.gii'\n",
    "        if not os.path.isfile(nfileL):\n",
    "            result=split_cii(fn=f2split,workbench_split_command=ab.workbench_split_command,resample=False)\n",
    "        else:\n",
    "            print(f'split files already exist subject {ab.subject}','Skipping..')\n",
    "        \n",
    "        return nfileL,nfileR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go though the early and late subjects, save the mean volume for each movie and then split it into surface giftis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break this task down and do it for each run. It is quite memory intensive given the size of the arrays we are reading in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  42 out of  42 | elapsed:   50.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE1_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE1_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,earlyvidsubs,0,earlydir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  42 out of  42 | elapsed:   53.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE2_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE2_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,earlyvidsubs,1,earlydir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  42 out of  42 | elapsed:   57.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE3_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE3_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,earlyvidsubs,2,earlydir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  42 out of  42 | elapsed:   50.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE4_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/early/tfMRI_MOVIE4_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,earlyvidsubs,3,earlydir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=20)]: Done 132 out of 132 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE1_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE1_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,latevidsubs,0,latedir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=20)]: Done 132 out of 132 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE2_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE2_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,latevidsubs,1,latedir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=20)]: Done 132 out of 132 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE3_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE3_PA_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,latevidsubs,2,latedir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=20)]: Done 132 out of 132 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE4_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_L.gii',\n",
       " '/tank/hedger/DATA/HCP_temp/late/tfMRI_MOVIE4_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries_sg_psc_R.gii')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=create_mean_data(dpaths,latevidsubs,3,latedir,True)\n",
    "mysub.split_dfile(x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
