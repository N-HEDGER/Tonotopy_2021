{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Speech-selective model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals.\n",
    "\n",
    "1. Make new design matrices, conditioned on the presence of speech.\n",
    "2. Generate two predictions, using the intial parameters estimated by the CSS model for both the speech and nonspeech design matrices.\n",
    "3. Find the linear combination of these parameter combinations that best explain the data.\n",
    "4. Use these beta-weights to test the predictions on the left out data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import h5py\n",
    "from funcs import h5_make, convolve, create_hrf, HCP_subject, masked_vert, h5_dump2\n",
    "from cfhcpy.base import AnalysisBase\n",
    "from prfpy.stimulus import PRFStimulus1Dn\n",
    "from prfpy.model import CSS_Iso1DGaussianModel\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data, as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of subject late on romulus with settings \n",
      "{\n",
      " \"identifier\": \"node230\",\n",
      " \"base_dir\": \"/scratch/2019/visual/hcp_{experiment}/\",\n",
      " \"code_dir\": \"/tank/hedger/scripts/HCP_tonotopy\",\n",
      " \"threads\": 40\n",
      "}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Prompt : After ref date? 1=True or 0=False 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.69s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating design matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/921 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 921/921 [00:00<00:00, 7036.04it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/921 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 921/921 [00:00<00:00, 7059.35it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:02<00:08,  2.67s/it]\n",
      "  0%|          | 0/918 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 918/918 [00:00<00:00, 7203.14it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/918 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 918/918 [00:00<00:00, 7162.59it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:05<00:05,  2.66s/it]\n",
      "  0%|          | 0/915 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 915/915 [00:00<00:00, 7213.26it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/915 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 915/915 [00:00<00:00, 7126.00it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:07<00:02,  2.66s/it]\n",
      "  0%|          | 0/901 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 901/901 [00:00<00:00, 7105.92it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/901 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 901/901 [00:00<00:00, 7287.27it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:10<00:00,  2.64s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making training and test folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "late = AnalysisBase()\n",
    "\n",
    "late.startup(subject='late', experiment_id='movie', yaml_file='/tank/hedger/software/hcp_movie/config.yml')\n",
    "\n",
    "\n",
    "late.subject_base_dir='/tank/hedger/DATA/HCP_temp/late'\n",
    "\n",
    "latesub=HCP_subject(late)\n",
    "\n",
    "latesub.prep_data()\n",
    "\n",
    "dtype='main' # Use the independent data, thereby chopping off the 'test sequence'\n",
    "standardise=True # Standardise the design matrix\n",
    "zaxis=1\n",
    "filt=False # Dont filter the design matrix,we will filter the predictions instead.\n",
    "\n",
    "latesub.import_data(dtype,standardise,filt,zaxis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for making speech design matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_speech_dm(fold,cut=True,captionpath='/tank/hedger/DATA/HCP_temp/Resources/captions'):\n",
    "    \n",
    "    mylist=sorted(os.listdir(captionpath)) # Path to the speech csvs.\n",
    "    \n",
    "    frame=pd.read_csv(os.path.join(captionpath,mylist[fold]),delimiter=';')\n",
    "    \n",
    "    frame['Startsecs']=frame['Start time in milliseconds']/1000 # Convert to seconds\n",
    "    frame['Endsecs']=frame['End time in milliseconds']/1000\n",
    "    \n",
    "    starts=np.array(frame['Startsecs'].astype(int)) \n",
    "    ends=np.array(frame['Endsecs'].astype(int))\n",
    "    event=np.zeros(latesub.ab.experiment_dict['run_durations'][fold])\n",
    "    \n",
    "    # Loop through the instances of speech as defined in the CSV \n",
    "    for i in range(starts.shape[0]):\n",
    "        event[starts[i]:ends[i]]=1 # Code instances of speech as 1.\n",
    "    if cut:\n",
    "        event=event[:-latesub.ab.experiment_dict['test_duration']]\n",
    "    other=1-event\n",
    "    other[latesub.dm_test[fold][0]==0]=0\n",
    "        \n",
    "    return empty,other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct all speech and nonspeech design matrices, organize them into the same fold combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_speech_dms(sub):\n",
    "    \n",
    "    speechdms_train,nspeechdms_train=[],[]\n",
    "    speechdms_test,nspeechdms_test=[],[]\n",
    "\n",
    "    # Go through each fold\n",
    "    for i in range(4):\n",
    "\n",
    "        # Make the design martices for each fold train and test. \n",
    "        dmtrain=np.hstack([make_speech_dm(t) for t in sub.folds[i]])\n",
    "        dmtest=make_speech_dm(i)\n",
    "    \n",
    "        # An intial step is to ensure that periods of silence remains silent.\n",
    "        # We so far defined the non-speech design matrix 1-speech, meaning that silence will be effected.\n",
    "        tempsp_train,tempnsp_train=np.copy(sub.dm_train[i]),np.copy(sub.dm_train[i])\n",
    "        tempsp_test,tempnsp_test=np.copy(sub.dm_test[i]),np.copy(sub.dm_test[i])\n",
    "        tempsp_train[:,dmtrain[0]==0]=0  \n",
    "        tempnsp_train[:,dmtrain[1]==0]=0\n",
    "        tempsp_test[:,dmtest[0]==0]=0\n",
    "        tempnsp_test[:,dmtest[1]==0]=0\n",
    "    \n",
    "    \n",
    "        speechdms_train.append(tempsp_train)\n",
    "        nspeechdms_train.append(tempnsp_train)\n",
    "    \n",
    "        speechdms_test.append(tempsp_test)\n",
    "        nspeechdms_test.append(tempnsp_test)\n",
    "        \n",
    "    sub.speechdms_train=speechdms_train\n",
    "    sub.nspeechdms_train=nspeechdms_train\n",
    "    sub.speechdms_test=speechdms_test\n",
    "    sub.nspeechdms_test=nspeechdms_test\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_speech_dms(latesub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Load in the CSS parameters estimated in notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename already exists\n",
      "filename already exists\n"
     ]
    }
   ],
   "source": [
    "# Load in the Mask that defines the tonotopic verties.\n",
    "frame=pd.read_csv('/tank/hedger/DATA/HCP_temp/OUTPUTS/CSVS/sframe.csv')\n",
    "mask=np.array(frame['mask'])\n",
    "nvertices=mask.shape[0]\n",
    "\n",
    "late=h5_make('/tank/hedger/DATA/HCP_temp/late','AUDITORY_FITS_CSS_FULL')\n",
    "latef = h5py.File(late, \"r\")\n",
    "latef.keys()\n",
    "late_foldp=np.array(latef['wb_log_fold_params_CSS'])\n",
    "\n",
    "\n",
    "early=h5_make('/tank/hedger/DATA/HCP_temp/early','AUDITORY_FITS_CSS_FULL')\n",
    "earlyf = h5py.File(early, \"r\")\n",
    "earlyf.keys()\n",
    "early_foldp=np.array(earlyf['wb_log_fold_params_CSS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "latef.close()\n",
    "earlyf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple function for asessing the predictions of the speech model against data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pred(tseries_raw,speechpred,nspeechpred,betas):\n",
    "    yhat=betas[-1]+(speechpred*betas[0])+(nspeechpred*betas[1])\n",
    "    rsq = 1-(yhat-tseries_raw).var(0)/tseries_raw.var(0)\n",
    "    return yhat,rsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for performing the fitting for the speech selective model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_speech_model(sub,mask,fold,foldp,log=True):\n",
    "    \n",
    "    indices=np.where(mask==1)[0]\n",
    "    blengths=np.array(sub.ab.experiment_dict['run_durations'])[np.array(sub.folds[fold])]-sub.ab.experiment_dict['test_duration']\n",
    "    blockarr=np.repeat(sub.folds[fold],blengths)\n",
    "    if log==True:\n",
    "        frequencies=np.log(sub.frequencies)\n",
    "        \n",
    "    # Make pRF stimulus from each speech and nonspeech design matrix.\n",
    "    speechstim_train=PRFStimulus1Dn(sub.speechdms_train[fold],frequencies,TR=1,block_inds=blockarr)\n",
    "    nspeechstim_train=PRFStimulus1Dn(sub.nspeechdms_train[fold],frequencies,TR=1,block_inds=blockarr)\n",
    "    speechstim_test=PRFStimulus1Dn(sub.speechdms_test[fold],frequencies,TR=1)\n",
    "    nspeechstim_test=PRFStimulus1Dn(sub.nspeechdms_test[fold],frequencies,TR=1)\n",
    "\n",
    "    # Use the same filter. \n",
    "    fparams = {\"window_length\":201,\"polyorder\": 3}\n",
    "\n",
    "    # Make corresponding prf models.\n",
    "    speechmod_train=CSS_Iso1DGaussianModel(speechstim_train,normalise_RFs=False,filter_predictions=True,filter_type='sg',filter_params=fparams)\n",
    "    speechmod_train.func='cart'\n",
    "\n",
    "    nspeechmod_train=CSS_Iso1DGaussianModel(nspeechstim_train,normalise_RFs=False,filter_predictions=True,filter_type='sg',filter_params=fparams)\n",
    "    nspeechmod_train.func='cart'\n",
    "\n",
    "    speechmod_test=CSS_Iso1DGaussianModel(speechstim_test,normalise_RFs=False,filter_predictions=True,filter_type='sg',filter_params=fparams)\n",
    "    speechmod_test.func='cart'\n",
    "\n",
    "    nspeechmod_test=CSS_Iso1DGaussianModel(nspeechstim_test,normalise_RFs=False,filter_predictions=True,filter_type='sg',filter_params=fparams)\n",
    "    nspeechmod_test.func='cart'\n",
    "\n",
    "    # Make the predictions based on CSS model for each design matrix\n",
    "    speechpreds=[speechmod_train.return_prediction(*list(foldp[fold,index,:][:-1]))[0] for index in indices]\n",
    "    nspeechpreds=[nspeechmod_train.return_prediction(*list(foldp[fold,index,:][:-1]))[0] for index in indices]\n",
    "\n",
    "    speechpreds_test=[speechmod_test.return_prediction(*list(foldp[fold,index,:][:-1]))[0] for index in indices]\n",
    "    nspeechpreds_test=[nspeechmod_test.return_prediction(*list(foldp[fold,index,:][:-1]))[0] for index in indices]\n",
    "    \n",
    "    yhats,betass,rsqs=[],[],[]\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Now do some fitting\n",
    "    i=0\n",
    "    for index in tqdm(indices):\n",
    "        \n",
    "        # Make design matrix (speech prediction, nspeech prediction, intercept).\n",
    "        dm=np.vstack([speechpreds[i],nspeechpreds[i],np.repeat(1,nspeechpreds[i].shape)])\n",
    "        \n",
    "        # Get the data\n",
    "        tseries_raw=sub.data_train[fold][:,index]\n",
    "        tseries_raw=np.nan_to_num(tseries_raw)\n",
    "        dm=np.nan_to_num(dm)\n",
    "        \n",
    "        # Solve the regression equation.\n",
    "        betas, _, _, _ = np.linalg.lstsq(dm.T, tseries_raw.T)\n",
    "        yhat = np.dot(betas.T, dm)\n",
    "        rsq = 1-(yhat-tseries_raw).var(0)/tseries_raw.var(0)\n",
    "    \n",
    "        # Save the betas.\n",
    "        rsqs.append(rsq)\n",
    "        yhats.append(yhat)\n",
    "        betass.append(betas)\n",
    "        i=i+1\n",
    "    \n",
    "    # Make into array the same shape as the full dataset. \n",
    "    R2=masked_vert(np.array(rsqs),mask)\n",
    "\n",
    "    betas=np.array(betass)\n",
    "    \n",
    "    # Test the predictions on the left out data.\n",
    "    i=0\n",
    "    perf=[]\n",
    "    for index in tqdm(indices):\n",
    "        \n",
    "        # We derive the out of sample predicitons from the linear combination of speech and nonspeech predictions\n",
    "        # that we just estimated.\n",
    "        \n",
    "        # We test such predictions on the left-out data to apply the same cross-validation strategy as for the CSS model.\n",
    "        res=test_pred(sub.data_test[fold][:,index],speechpreds_test[i],nspeechpreds_test[i],betass[i])\n",
    "        \n",
    "        perf.append(res[1])\n",
    "        i=i+1\n",
    "        \n",
    "    xval=masked_vert(np.array(perf),mask)\n",
    "    \n",
    "    # Save out the R2, out of sample performance and beta weights.\n",
    "    return R2,xval,betas \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_sms=[]\n",
    "\n",
    "for i in tqdm(range(4)):\n",
    "    late_sms.append(fit_speech_model(latesub,mask,i,late_foldp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out the outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_diff=np.array([late_sms[i][1]-late_foldp[i,:,-1] for i in range(4)])\n",
    "late_sp_xval=np.array([late_sms[i][1] for i in range(4)])\n",
    "late_sp_betas=np.array([masked_vert(late_sms[i][2][:,0],mask) for i in range(4)])\n",
    "late_nsp_betas=np.array([masked_vert(late_sms[i][2][:,1],mask) for i in range(4)])\n",
    "late_intercepts=np.array([masked_vert(late_sms[i][2][:,2],mask) for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ub_log_fold_intercepts'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late=h5_make('/tank/hedger/DATA/HCP_temp/late','AUDITORY_FITS_CSS_FULL')\n",
    "h5_dump2(late,late_diff,'ub_log_fold_sp_diff')\n",
    "h5_dump2(late,late_sp_xval,'ub_log_fold_sp_xval')\n",
    "h5_dump2(late,late_sp_betas,'ub_log_fold_sp_betas')\n",
    "h5_dump2(late,late_nsp_betas,'ub_log_fold_nsp_betas')\n",
    "h5_dump2(late,late_intercepts,'ub_log_fold_intercepts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for the early subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early = AnalysisBase()\n",
    "\n",
    "early.startup(subject='early', experiment_id='movie', yaml_file='/tank/hedger/software/hcp_movie/config.yml')\n",
    "\n",
    "\n",
    "early.subject_base_dir='/tank/hedger/DATA/HCP_temp/early'\n",
    "\n",
    "earlysub=HCP_subject(early)\n",
    "\n",
    "earlysub.prep_data()\n",
    "\n",
    "dtype='main' # Use the independent data, thereby chopping off the 'test sequence'\n",
    "standardise=True # Standardise the design matrix\n",
    "zaxis=1\n",
    "filt=False # Dont filter the design matrix,we will filter the predictions instead.\n",
    "\n",
    "earlysub.import_data(dtype,standardise,filt,zaxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_speech_dms(earlysub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_sms=[]\n",
    "\n",
    "for i in tqdm(range(4)):\n",
    "    early_sms.append(fit_speech_model(earlysub,mask,i,early_foldp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_diff=np.array([early_sms[i][1]-early_foldp[i,:,-1] for i in range(4)])\n",
    "early_sp_xval=np.array([early_sms[i][1] for i in range(4)])\n",
    "early_sp_betas=np.array([masked_vert(early_sms[i][2][:,0],mask) for i in range(4)])\n",
    "early_nsp_betas=np.array([masked_vert(early_sms[i][2][:,1],mask) for i in range(4)])\n",
    "early_intercepts=np.array([masked_vert(early_sms[i][2][:,2],mask) for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ub_log_fold_intercepts'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early=h5_make('/tank/hedger/DATA/HCP_temp/early','AUDITORY_FITS_CSS_FULL')\n",
    "\n",
    "h5_dump2(early,early_diff,'ub_log_fold_sp_diff')\n",
    "h5_dump2(early,early_sp_xval,'ub_log_fold_sp_xval')\n",
    "h5_dump2(early,early_sp_betas,'ub_log_fold_sp_betas')\n",
    "h5_dump2(early,early_nsp_betas,'ub_log_fold_nsp_betas')\n",
    "h5_dump2(early,early_intercepts,'ub_log_fold_intercepts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "latef.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
